Job Configuration
=================

Klio-specific and :ref:`user-specified custom <custom-conf>` job configuration.

.. option:: job_config.allow_non_klio_messages BOOL

    Allow this job to process free-form, non-``KlioMessage`` messages.

    **Default**: ``False``


.. option:: job_config.blocking BOOL

    Wait for Dataflow job to finish before exiting.

    **Default**: ``False``


.. _custom-conf:
.. option:: job_config.<additional_key> ANY

    Arbitrary key-value pairs for any custom configuration specific to a job. These will be
    attached to the ``job_config`` attribute of ``KlioConfig`` object.

    See :ref:`access-config` on how to access configuration within a pipeline or transform.


``job_config.events``
---------------------

    Event inputs/outputs designate where to read/write KlioMessages.

    The :doc:`KlioMessage <../pipeline/message>` contains a unique identifier of somesort that
    refers to a unit of work (e.g. file IDs, track IDs, etc.). This unique identifier can then be
    used to look up the binary data as configured in ``job_config.data`` for the job to process. A
    job's events can therefore be seen as "triggers" of work needing to be done on particular
    binary data.

    Example:

        .. code-block:: yaml

            name: my-cool-job
            pipeline_options:
              streaming: True
            job_config:
              events:
                inputs:
                  - type: pubsub
                    subscription: my-input-subscription
                outputs:
                  - type: pubsub
                    topic: my-output-topic


``job_config.events.inputs[]``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    A list of input configuration that will be will used to determine when and how to do work.

    If more than one input is configured, please familiarized yourself with
    :doc:`how multiple configured inputs <../pipeline/multiple_inputs>` are handled in Klio.


.. option:: job_config.events.inputs[].type STR

    Type of input the job is reading events from.

    See :doc:`event_config` for the supported configuration by type.

    | **Streaming Options**: ``pubsub``, ``custom``
    | **Batch Options**: ``file``, ``avro``, ``bigquery``, ``custom``


.. option:: job_config.events.inputs[].<type_specific_config>

    See :doc:`event_config` for the supported configuration by type.


.. _skip-klio-read:
.. option:: job_config.events.inputs[].skip_klio_read BOOL

    Klio will automatically read from the configured input unless this value is set to ``True``.

    If all declared inputs are configured to skip Klio's automatic reading from Pub/Sub, the
    `Pipeline`_ object will then be given to the job's ``run.py::run`` function instead of a
    `PCollection`_ object. In this case, you must implement the reading behavior (i.e. using a
    different Beam I/O transform).

    Useful for implementing different behavior than the default, or to toggle off multiple
    inputs.

    **Default**: ``False``


``job_config.events.outputs[]``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    A list of output configurations that Klio will use to signify that work has been
    completed.

    .. warning::

        Currently, only one event output configuration is supported in Klio out of the box.

        If more than one output is required, set ``skip_klio_write`` of each output configuration
        to ``True``.


.. option:: job_config.events.outputs[].type STR

    Type of output the job is writing events to.

    See :doc:`event_config` for the supported configuration by type.

    | **Streaming Options**: ``pubsub``, ``custom``
    | **Batch Options**: ``file``, ``bigquery``, ``custom``


.. option:: job_config.events.outputs[].<type_specific_config>

    See :doc:`event_config` for the supported configuration by type.


.. _skip-klio-write:
.. option:: job_config.events.outputs[].skip_klio_write BOOL

    Klio will automatically write to this output topic unless this value is set to ``True``.

    Useful for implementing different behavior than the default, using multiple outputs, or to
    toggle off event output.

    **Default**: ``False``


``job_config.data``
-------------------

    Data inputs/outputs refer to where the files are (typically GCS buckets) that ``KlioMessages``
    generated by event inputs refer to.


``job_config.data.inputs[]``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    A list of input configurations that Klio will use to look for data to be processed.

    By default, Klio will drop a ``KlioMessage`` when input data for the corresponding element ID
    does not exist. Set ``skip_klio_existence_check`` to ``False`` to implement different behavior.

    .. note::

        Klio does not upload data automatically to the configured location. This must be done from
        within the pipeline.

    .. warning::

        Currently, only one data input configuration is supported in Klio out of the box.

        If more than one input is required, set ``skip_klio_existence_check`` of each input
        configuration to ``True``.


.. option:: job_config.data.inputs[].type STR

    Type of input the job is reading data from.

    See :doc:`data_config` for the supported configuration by type.

    **Options**: ``gcs``, ``custom``


.. option:: job_config.data.inputs[].<type_specific_config>

    See :doc:`data_config` for the supported configuration by type.

.. _ping-mode:
.. option:: job_config.data.inputs[].ping BOOL

    Set a global ping mode of ``KlioMessages``.

    When ``True``, ping mode will not trigger transforms for messages and send it directly to
    configured event output.

    If ``ping`` is set on an individual ``KlioMessage`` - whether ``True`` or ``False`` - that
    setting will be preferred over this global setting.

    **Default**: ``False``

.. _skip-input-ext-check:
.. option:: job_config.data.inputs[].skip_klio_existence_check BOOL

    Tell Klio to skip its default input data existence check. Set this to ``True`` when input
    data existence checks are not needed, or to implement behavior different than the default.

    :doc:`Read more <../pipeline/transforms>` about how Klio performs these data existence checks.

    **Default**: ``False``


``job_config.data.outputs[]``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    A list of output configurations that Klio will use to look for data that has already been
    processed.

    .. note::

        Klio does not upload data automatically to the configured location. This must be done from
        within the pipeline.

    .. warning::

        Currently, only one data output configuration is supported in Klio out of the box.

        If more than one output is required, set ``skip_klio_existence_check`` of each output
        configuration to ``True``.



.. option:: job_config.data.outputs[].type STR

    Type of output the job is writing data to.

    See :doc:`data_config` for the supported configuration by type.

    **Options**: ``gcs``, ``custom``


.. option:: job_config.data.outputs[].<type_specific_config>

    See :doc:`data_config` for the supported configuration by type.


.. _force-mode:
.. option:: job_config.data.outputs[].force BOOL

    Set a global force of ``KlioMessages`` if output data already exists.

    When ``True``, force mode will force the pipeline to process work when its corresponding
    output data already exists.

    If ``force`` is set on an individual ``KlioMessage`` - whether ``True`` or ``False`` - that
    setting will be preferred over this global setting.

    **Default**: ``False``


.. _skip-output-ext-check:
.. option:: job_config.data.outputs[].skip_klio_existence_check BOOL

    Tell Klio to skip its default output data existence check. Set this to ``True`` when output
    data existence checks are not needed, or to implement behavior different than the default.

    :doc:`Read more <../pipeline/transforms>` about how Klio performs these data existence checks.

    **Default**: ``False``


``job_config.metrics``
----------------------

    With no additional configuration needed, metrics will be turned on and collected. The default
    client depends on the runner:

    | **DataflowRunner**: Stackdriver log-based metrics
    | **DirectRunner**: Python standard library logging

    See :doc:`documentation on metrics <../pipeline/metrics>` for information on how to emit metrics from a pipeline.


.. option:: job_config.metrics.logger DICT | BOOL

    Default metrics client on ``DirectRunner``. To turn it off, set this key to ``False``. To
    adjust its configuration, use the properties ``level`` and ``timer_unit``.


.. option:: job_config.metrics.logger.level STR

    Log level at which metrics are emitted.

    | **Options**: ``debug``, ``info``, ``warning``, ``error``, ``critical``
    | **Default**: ``debug``


.. option:: job_config.metrics.logger.timer_unit STR

    Globally set the default unit of time for timers.

    | **Options**: ``ns``, ``nanoseconds``, ``us``, ``microseconds``, ``ms``, ``milliseconds``,
     ``s``, ``seconds``
    | **Default**: ``ns``


.. option:: job_config.metrics.stackdriver DICT | BOOL

    Default metrics client on ``DataflowRunner``. To turn it off, set this key to ``False``. To
    adjust its configuration, use the properties ``level`` and ``timer_unit``.


.. option:: job_config.metrics.stackdriver.level STR

    Log level at which metrics are emitted.

    | **Options**: ``debug``, ``info``, ``warning``, ``error``, ``critical``
    | **Default**: ``debug``


.. option:: job_config.metrics.stackdriver.timer_unit STR

    Globally set the default unit of time for timers.

    | **Options**: ``ns``, ``nanoseconds``, ``us``, ``microseconds``, ``ms``, ``milliseconds``,
     ``s``, ``seconds``
    | **Default**: ``ns``


.. _Pipeline: https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline
.. _PCollection: https://beam.apache.org/documentation/programming-guide/#pcollections
